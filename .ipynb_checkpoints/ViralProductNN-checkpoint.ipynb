{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5c0953e0-b0ea-462c-93bf-d431ba75f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "\n",
    "df = pd.read_csv('ProductHuntProducts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce5fdbc6-fe9e-4ba0-824a-88cfcc971be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>votesCount</th>\n",
       "      <th>commentsCount</th>\n",
       "      <th>tagline</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>description</th>\n",
       "      <th>topics.totalCount</th>\n",
       "      <th>topics.nodesPieces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>467006</td>\n",
       "      <td>Pieces Copilot+</td>\n",
       "      <td>432</td>\n",
       "      <td>184</td>\n",
       "      <td>Remember anything with a real-time, on-device ...</td>\n",
       "      <td>2024-07-10T07:01:00Z</td>\n",
       "      <td>Ask questions about anything on your desktop c...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'Productivity'}, {'name': 'Software ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>466991</td>\n",
       "      <td>Turf</td>\n",
       "      <td>417</td>\n",
       "      <td>187</td>\n",
       "      <td>The in-app community platform for SaaS companies</td>\n",
       "      <td>2024-07-10T07:01:00Z</td>\n",
       "      <td>Create community forums, customer feedback boa...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'Customer Success'}, {'name': 'SaaS'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>466593</td>\n",
       "      <td>StreamAlive App for Zoom</td>\n",
       "      <td>315</td>\n",
       "      <td>162</td>\n",
       "      <td>Just put it in the chat</td>\n",
       "      <td>2024-07-10T07:01:00Z</td>\n",
       "      <td>Run polls, quizzes, word clouds, spinner wheel...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'SaaS'}, {'name': 'Remote Work'}, {'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>466672</td>\n",
       "      <td>Flat</td>\n",
       "      <td>230</td>\n",
       "      <td>99</td>\n",
       "      <td>Simple, delightful, collaborative work tracking</td>\n",
       "      <td>2024-07-10T07:01:00Z</td>\n",
       "      <td>Forget bloated project management tools. Flat ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'Productivity'}, {'name': 'Task Mana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>469161</td>\n",
       "      <td>Leo</td>\n",
       "      <td>211</td>\n",
       "      <td>73</td>\n",
       "      <td>AI phone assistants</td>\n",
       "      <td>2024-07-10T07:01:00Z</td>\n",
       "      <td>Quickly set up AI phone assistants for making ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'Artificial Intelligence'}, {'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7168</th>\n",
       "      <td>455400</td>\n",
       "      <td>Dhime</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Learn Dance with AI</td>\n",
       "      <td>2024-05-05T07:01:00Z</td>\n",
       "      <td>Learn dance anywhere anytime with Dhime, your ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[{'name': 'Artificial Intelligence'}, {'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7169</th>\n",
       "      <td>455373</td>\n",
       "      <td>FaceX</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Face X Anything, become a NPC, or Anime Character</td>\n",
       "      <td>2024-05-05T15:01:00Z</td>\n",
       "      <td>FaceX, mean's Face X Anything.(Optimization Pa...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'Design Tools'}, {'name': 'Art'}, {'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7170</th>\n",
       "      <td>454288</td>\n",
       "      <td>CopyBuddy.io</td>\n",
       "      <td>119</td>\n",
       "      <td>42</td>\n",
       "      <td>Send cold emails your prospects love respondin...</td>\n",
       "      <td>2024-05-05T07:01:00Z</td>\n",
       "      <td>Send cold emails your prospects love respondin...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'Sales'}, {'name': 'Writing'}, {'nam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>455478</td>\n",
       "      <td>RandCall</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Random Match Chat</td>\n",
       "      <td>2024-05-06T01:49:40Z</td>\n",
       "      <td>Randcall —— Talking &amp; Meeting New People Get t...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'Messaging'}, {'name': 'Video Stream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7172</th>\n",
       "      <td>450031</td>\n",
       "      <td>MTlogin</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Ultimate Anti-Detection Fingerprint Simulation</td>\n",
       "      <td>2024-05-06T02:14:38Z</td>\n",
       "      <td>MTLogin browser offers advanced fingerprint se...</td>\n",
       "      <td>3</td>\n",
       "      <td>[{'name': 'Marketing'}, {'name': 'Affiliate ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7133 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                      name  votesCount  commentsCount  \\\n",
       "0     467006           Pieces Copilot+         432            184   \n",
       "1     466991                      Turf         417            187   \n",
       "2     466593  StreamAlive App for Zoom         315            162   \n",
       "3     466672                      Flat         230             99   \n",
       "4     469161                       Leo         211             73   \n",
       "...      ...                       ...         ...            ...   \n",
       "7168  455400                     Dhime           5              3   \n",
       "7169  455373                     FaceX           2              1   \n",
       "7170  454288              CopyBuddy.io         119             42   \n",
       "7171  455478                  RandCall           2              2   \n",
       "7172  450031                   MTlogin           3              1   \n",
       "\n",
       "                                                tagline             createdAt  \\\n",
       "0     Remember anything with a real-time, on-device ...  2024-07-10T07:01:00Z   \n",
       "1      The in-app community platform for SaaS companies  2024-07-10T07:01:00Z   \n",
       "2                               Just put it in the chat  2024-07-10T07:01:00Z   \n",
       "3       Simple, delightful, collaborative work tracking  2024-07-10T07:01:00Z   \n",
       "4                                   AI phone assistants  2024-07-10T07:01:00Z   \n",
       "...                                                 ...                   ...   \n",
       "7168                                Learn Dance with AI  2024-05-05T07:01:00Z   \n",
       "7169  Face X Anything, become a NPC, or Anime Character  2024-05-05T15:01:00Z   \n",
       "7170  Send cold emails your prospects love respondin...  2024-05-05T07:01:00Z   \n",
       "7171                                  Random Match Chat  2024-05-06T01:49:40Z   \n",
       "7172     Ultimate Anti-Detection Fingerprint Simulation  2024-05-06T02:14:38Z   \n",
       "\n",
       "                                            description  topics.totalCount  \\\n",
       "0     Ask questions about anything on your desktop c...                  3   \n",
       "1     Create community forums, customer feedback boa...                  3   \n",
       "2     Run polls, quizzes, word clouds, spinner wheel...                  3   \n",
       "3     Forget bloated project management tools. Flat ...                  3   \n",
       "4     Quickly set up AI phone assistants for making ...                  3   \n",
       "...                                                 ...                ...   \n",
       "7168  Learn dance anywhere anytime with Dhime, your ...                  2   \n",
       "7169  FaceX, mean's Face X Anything.(Optimization Pa...                  3   \n",
       "7170  Send cold emails your prospects love respondin...                  3   \n",
       "7171  Randcall —— Talking & Meeting New People Get t...                  3   \n",
       "7172  MTLogin browser offers advanced fingerprint se...                  3   \n",
       "\n",
       "                                     topics.nodesPieces  \n",
       "0     [{'name': 'Productivity'}, {'name': 'Software ...  \n",
       "1     [{'name': 'Customer Success'}, {'name': 'SaaS'...  \n",
       "2     [{'name': 'SaaS'}, {'name': 'Remote Work'}, {'...  \n",
       "3     [{'name': 'Productivity'}, {'name': 'Task Mana...  \n",
       "4     [{'name': 'Artificial Intelligence'}, {'name':...  \n",
       "...                                                 ...  \n",
       "7168  [{'name': 'Artificial Intelligence'}, {'name':...  \n",
       "7169  [{'name': 'Design Tools'}, {'name': 'Art'}, {'...  \n",
       "7170  [{'name': 'Sales'}, {'name': 'Writing'}, {'nam...  \n",
       "7171  [{'name': 'Messaging'}, {'name': 'Video Stream...  \n",
       "7172  [{'name': 'Marketing'}, {'name': 'Affiliate ma...  \n",
       "\n",
       "[7133 rows x 9 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset='id', keep=\"first\", inplace=True)\n",
    "# df['target'] = (df['votesCount'] + df['commentsCount']).apply(lambda x: min(x, 1))\n",
    "\n",
    "# # Check for missing values in description and title\n",
    "# df.dropna(subset=['description', 'tagline'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87d720d3-561a-4cfa-8342-33e695cb2cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (4.35.2)\n",
      "Requirement already satisfied: torch in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from torch) (4.6.2)\n",
      "Requirement already satisfied: fsspec in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ivanhusarov/miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fbc9994-e0c8-403b-84e0-e62fd96920d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanhusarov/miniconda3/lib/python3.10/site-packages/pandas/core/internals/blocks.py:366: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['target'] = df['votesCount'] + df['commentsCount']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['target'] = scaler.fit_transform(np.maximum(0, np.log(df[['target']])))\n",
    "\n",
    "df['target'].plot(kind='hist', bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24d57c03-af4a-41c7-a168-9580ab1290b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'] = df['description'].fillna('na').astype(str).str.lower()\n",
    "df['tagline'] = df['tagline'].fillna('na').astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a818cab4-e251-4197-9c5f-141267844756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "35699245-bb6b-43be-8cc7-eae1d4c8d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_bert_embedding(df.iloc[3509]['description'])\n",
    "# df.iloc[[3509]]['description']\n",
    "# get_bert_embedding(\"na\")\n",
    "# df['description'] = df.iloc[[3509]]['description'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79971d81-d578-43e3-b50b-3ad653cd4807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ask questions about anything on your desktop c...\n",
       "1       create community forums, customer feedback boa...\n",
       "2       run polls, quizzes, word clouds, spinner wheel...\n",
       "3       forget bloated project management tools. flat ...\n",
       "4       quickly set up ai phone assistants for making ...\n",
       "                              ...                        \n",
       "7168    learn dance anywhere anytime with dhime, your ...\n",
       "7169    facex, mean's face x anything.(optimization pa...\n",
       "7170    send cold emails your prospects love respondin...\n",
       "7171    randcall —— talking & meeting new people get t...\n",
       "7172    mtlogin browser offers advanced fingerprint se...\n",
       "Name: description, Length: 7133, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.iloc[3509]['description']\n",
    "# df[['description', 'tagline']]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9dd758d7-cc51-4e54-b253-42edbf008dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing taglines:   0%|                                                                                                                                                              | 0/7133 [00:00<?, ?it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|                                                                                                                                                      | 2/7133 [00:00<07:17, 16.31it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|                                                                                                                                                      | 4/7133 [00:00<06:49, 17.40it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▏                                                                                                                                                     | 6/7133 [00:00<07:55, 14.99it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▏                                                                                                                                                     | 9/7133 [00:00<06:50, 17.34it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▏                                                                                                                                                    | 11/7133 [00:00<06:40, 17.78it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▎                                                                                                                                                    | 14/7133 [00:00<06:17, 18.85it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▎                                                                                                                                                    | 16/7133 [00:00<06:16, 18.93it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▍                                                                                                                                                    | 19/7133 [00:01<06:05, 19.48it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▍                                                                                                                                                    | 22/7133 [00:01<06:00, 19.74it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▌                                                                                                                                                    | 24/7133 [00:01<06:04, 19.50it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▌                                                                                                                                                    | 26/7133 [00:01<06:14, 18.98it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▌                                                                                                                                                    | 28/7133 [00:01<06:11, 19.12it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▋                                                                                                                                                    | 30/7133 [00:01<06:21, 18.60it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▋                                                                                                                                                    | 32/7133 [00:01<06:26, 18.37it/s]\u001b[A\n",
      "Tokenizing taglines:   0%|▋                                                                                                                                                    | 34/7133 [00:01<06:27, 18.30it/s]\u001b[A\n",
      "Tokenizing taglines:   1%|▊                                                                                                                                                    | 36/7133 [00:01<06:25, 18.43it/s]\u001b[A\n",
      "Tokenizing taglines:   1%|▊                                                                                                                                                    | 38/7133 [00:02<06:25, 18.38it/s]\u001b[A\n",
      "Tokenizing taglines:   1%|▊                                                                                                                                                    | 40/7133 [00:02<06:24, 18.44it/s]\u001b[A\n",
      "Tokenizing taglines:   1%|▉                                                                                                                                                    | 42/7133 [00:02<06:23, 18.49it/s]\u001b[A\n",
      "Tokenizing taglines:   1%|▉                                                                                                                                                    | 44/7133 [00:02<06:17, 18.76it/s]\u001b[A\n",
      "Tokenizing taglines:   1%|▉                                                                                                                                                    | 46/7133 [00:02<06:14, 18.91it/s]\u001b[A\n",
      "Tokenizing taglines:   1%|█                                                                                                                                                    | 48/7133 [00:02<06:17, 18.77it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get embeddings for descriptions and titles\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# description_embeddings = [get_bert_embedding(desc) for desc in df['description']]\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tagline_embeddings \u001b[38;5;241m=\u001b[39m [get_bert_embedding(tagline) \u001b[38;5;28;01mfor\u001b[39;00m tagline \u001b[38;5;129;01min\u001b[39;00m tqdm(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagline\u001b[39m\u001b[38;5;124m'\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenizing taglines\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[121], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get embeddings for descriptions and titles\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# description_embeddings = [get_bert_embedding(desc) for desc in df['description']]\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m tagline_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[43mget_bert_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtagline\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tagline \u001b[38;5;129;01min\u001b[39;00m tqdm(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagline\u001b[39m\u001b[38;5;124m'\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenizing taglines\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[38], line 17\u001b[0m, in \u001b[0;36mget_bert_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/pytorch_utils.py:241\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:451\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 451\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Get embeddings for descriptions and titles\n",
    "# description_embeddings = [get_bert_embedding(desc) for desc in df['description']]\n",
    "tagline_embeddings = [get_bert_embedding(tagline) for tagline in tqdm(df['tagline'], desc='Tokenizing taglines')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede38f2-8585-4184-acbd-def60d1771d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine embeddings\n",
    "embeddings = [torch.cat((desc_emb, tag_emb), dim=1) for desc_emb, tag_emb in zip(description_embeddings, tagline_embeddings)]\n",
    "embeddings = torch.cat(embeddings, dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
